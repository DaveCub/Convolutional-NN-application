{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The MNIST dataset is an acronym that stands for the Modified National Institute of Standards and Technology dataset.\n",
        "It is a dataset of 60,000 small square 28×28 pixel grayscale images of handwritten single digits between 0 and 9.\n"
      ],
      "metadata": {
        "id": "fdsY8V1Z6Ux1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- import libraries --------------------------------------------------\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import argmax\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.datasets import mnist\t\t\t#import MNIST dataset\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from google.colab import files\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "trai=1      # 0-not to train model...........1- train model again\n",
        "pred=0      # 0-not to predict new image.....1-predit new image"
      ],
      "metadata": {
        "id": "W7esf1hGrtst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load train and test dataset\n",
        "The first element, trainX.shape[0], represents the size or length of the first dimension of the trainX array. Assuming trainX is a NumPy array or a similar data structure, the shape attribute returns a tuple that describes the dimensions of the array. The index [0] is used to access the size of the first dimension specifically. 28,28,1 (single channel for B/W, 28,28,3 for color image)\n",
        "\n",
        "to_categorical() is being used to convert the labels in testY to categorical format. The to_categorical() function is commonly used in frameworks like Keras or TensorFlow to perform this conversion. It takes an array of numerical labels and returns an array of one-hot encoded vectors representing the same labels in categorical format. Each one-hot vector will have a 1 in the position corresponding to the class label and 0s elsewhere."
      ],
      "metadata": {
        "id": "nAmmo0xfr1bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "\t(trainX, trainY), (testX, testY) = mnist.load_data()   # load dataset\n",
        "\n",
        "\t#to see size of X(features) and its labels (Y)\n",
        "\t#print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "\t#print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))  # reshape dataset to have a single color channel\n",
        "\ttestX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "\ttrainY = to_categorical(trainY)                        # one hot encode target values\n",
        "\ttestY = to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY"
      ],
      "metadata": {
        "id": "O6JakpDKrwLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing Data"
      ],
      "metadata": {
        "id": "d76GgfzF1Pc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the data---- scale pixels  -------------------------------------------------------\n",
        "def prep_pixels(train, test):\n",
        "\ttrain_norm = train.astype('float32')  # convert from integers to floats\n",
        "\ttest_norm = test.astype('float32')\n",
        "\ttrain_norm = train_norm / 255.0       # normalized or scaled to range 0-1\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\treturn train_norm, test_norm          # return normalized images for train and test"
      ],
      "metadata": {
        "id": "8Zkhs_2lsfJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define model**\n",
        "\n",
        "Conv2D: This is the function or layer used to add a 2D convolutional layer to the model.\n",
        "\n",
        "32: This parameter specifies the number of filters or convolutional kernels to be applied in the layer. In this case, there will be 32 filters, and each filter will learn different features from the input data.\n",
        "\n",
        "(3, 3): This parameter specifies the size of each filter. In this case, the filters will have a size of 3x3 pixels.\n",
        "\n",
        "activation='relu': This parameter sets the activation function to be applied after the convolution operation. In this case, the Rectified Linear Unit (ReLU) activation function is used, which introduces non-linearity to the model.\n",
        "\n",
        "kernel_initializer='random_uniform':Initializers define the way to set the initial random weights of Keras layers. This parameter sets the initialization method for the filter weights. In this case, the filter weights are randomly initialized from a uniform distribution.\n",
        "\n",
        "input_shape=(28, 28, 1): This parameter specifies the shape of the input data that will be fed into the convolutional layer. The input shape is set as (28, 28, 1), indicating that the input images have a height and width of 28 pixels and a single channel (grayscale images). The input shape is important for defining the input dimensions of the first layer in the model.\n",
        "\n",
        "Overall, this line of code adds a convolutional layer with 32 filters of size 3x3, followed by ReLU activation, random weight initialization, and an input shape of (28, 28, 1) to a neural network model.\n",
        "\n",
        "\n",
        "**MaxPooling2D**: This is the function or layer used to add a 2D max pooling layer to the model.\n",
        "(2, 2): This parameter specifies the size of the pooling window or pool size. In this case, the pooling window is 2x2 pixels. The pooling window slides over the input feature maps, and the maximum value within each window is selected.\n",
        "The MaxPooling2D layer reduces the spatial dimensions of the input feature maps while retaining the most significant features. It divides the feature maps into non-overlapping regions defined by the pool size and selects the maximum value within each region.\n",
        "\n",
        "By specifying (2, 2) as the pool size, the spatial dimensions of the feature maps will be reduced by a factor of 2. For example, if the input feature maps have dimensions of 28x28, the output feature maps after the pooling layer will have dimensions of 14x14.\n",
        "\n",
        "Max pooling layers are commonly used in convolutional neural networks to downsample the feature maps, reduce overfitting, and extract the most relevant features. They help in capturing important spatial information while reducing the computational complexity of the model.\n",
        "\n",
        "Flatten Layer: After the max pooling layer, the flatten layer is used to convert the multidimensional feature maps into a flattened vector. It reshapes the output of the previous layers into a 1D vector, which can be fed into a fully connected layer. The flatten layer preserves the spatial relationships learned by the convolutional layers but transforms the data into a format suitable for traditional dense (fully connected) layers.\n",
        "\n",
        "The purpose of this sequence is to progressively extract higher-level features from the input data while reducing the spatial dimensions. The convolutional layer detects local patterns, edges, and textures, while the max pooling layer captures the most prominent features. The flatten layer then prepares the data for further processing by fully connected layers, which can learn complex relationships and make predictions.\n",
        "\n",
        "Dense layer:\n",
        "In a dense layer, each neuron is connected to every neuron in the previous layer.\n",
        "A fully connected (dense) layer with 100 units is added. It uses ReLU activation and random uniform weight initialization. The final dense layer with 10 units is added, representing the output layer. It uses the softmax activation function, which is commonly used for multi-class classification problems.\n",
        "\n",
        "Stochastic Gradient Descent (SGD) optimization algorithm\n",
        "\n",
        "learning_rate=0.01: The learning rate determines the step size at which the optimizer updates the weights during training. A higher learning rate can lead to faster convergence but may risk overshooting the optimal solution, while a lower learning rate can help converge more accurately but may take longer.\n",
        "\n",
        "Momentum helps accelerate the learning process by accumulating the past gradients and incorporating them into the current gradient update. It adds a fraction (0.9 in this case) of the previous update to the current update, allowing the optimizer to navigate through flatter regions of the optimization landscape more efficiently.\n",
        "\n",
        "The learning rate determines the step size, and momentum helps to smooth out the optimization process."
      ],
      "metadata": {
        "id": "UoZXF6qw6b6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---- define cnn model --------------------------------------------------------\n",
        "def define_model():\n",
        "\tmodel = Sequential()                   # sequential type model\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='random_uniform', input_shape=(28, 28, 1))) # input layer (convolutional)\n",
        "\tmodel.add(MaxPooling2D((2, 2)))        # Downsamples input dimension by taking maximum value over an input window\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='random_uniform')) # 2nd (convolutional) layer\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='random_uniform')) # 3rd (convolutional) layer\n",
        "\tmodel.add(MaxPooling2D((2, 2)))        # Downsamples input dimension by taking maximum value over an input window\n",
        "\tmodel.add(Flatten())                   # Flatten layer\n",
        "\tmodel.add(Dense(100, activation='relu', kernel_initializer='random_uniform'))            # core layer\n",
        "\tmodel.add(Dense(10, activation='softmax'))                                           # output layer (dimension=10)\n",
        "\t# compile model\n",
        "\n",
        "\n",
        "\topt = SGD(learning_rate=0.01, momentum=0.9)                                          # select optimiter type\n",
        "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])  # compile model\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "Japj8k4usgoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.fit()Trains the model for a fixed number of epochs.\n",
        "\n",
        "In order to estimate the performance of a model for a given training run, we can further split the training set into a train and validation dataset. Performance on the train and validation dataset over each run can then be plotted to provide learning curves and insight into how well a model is learning the problem. The Keras API supports this by specifying the “validation_data” argument to the model.fit()\n",
        "\n",
        "history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=2)\n",
        "Trains the model for a fixed number of epochs (dataset iterations).\n",
        "\n",
        "batch_size: Number of samples per gradient update\n",
        "•\tWe divide the training set into batches (number of samples). The batch_size is the sample size (number of training instances each batch contains). The number of batches is obtained by:  #of batches=(size of the train dataset/batchsize)+1\n",
        "\n",
        "Batches and steps. Considering batch_size=128\n",
        "•\tAccording to the above equation, here we get 469 (60,000 / 128 + 1) batches. We add 1 to compensate for any fractional part.\n",
        "•\tIn one epoch, the fit()method process 469 steps. The model parameters will be updated 469 times in each epoch of optimization.\n",
        "•\tThe algorithm takes the first 128 training instances in each epoch and updates the model parameters.\n",
        "•\tThen, it takes the next 128 training instances and updates the model parameters. The algorithm will do this process until 469 steps are complete in each epoch.\n",
        "\n",
        "\n",
        "Epochs: Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided.\n",
        "\n",
        "verbose:  'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
        "validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data.\n",
        "\n",
        "**K-Folds cross-validator.** Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds\n",
        "random_state=1: This parameter sets the random seed for reproducibility. By setting a specific random seed, the shuffling and fold splitting process will yield the same results each time the code is executed, which is useful for reproducible experiments.\n",
        "\n",
        "On the for loop:The test set for each fold will be used to evaluate the model both during each epoch of the training run, so that we can later create learning curves, and at the end of the run, so that we can estimate the performance of the model. As such, we will keep track of the resulting history from each run, as well as the classification accuracy of the fold.\n",
        "\n",
        "The line _, acc = model.evaluate(testX, testY, verbose=0) evaluates the performance of a trained model on a test dataset and assigns the accuracy value to the variable acc.\n",
        "\n",
        "model: Refers to the trained neural network model that you want to evaluate.\n",
        "testX: Represents the input features of the test dataset.\n",
        "testY: Represents the corresponding target labels of the test dataset.\n",
        "verbose=0: The verbose parameter controls the verbosity of the evaluation process. Setting it to 0 means no progress or log messages will be displayed during evaluation.\n",
        "The model.evaluate() function computes the loss and metrics (in this case, accuracy) of the model on the provided test dataset. It returns a list containing the computed loss value and the values of the specified metrics. In this case, the accuracy value is extracted and assigned to the acc variable."
      ],
      "metadata": {
        "id": "4DI4L_xPtBRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- evaluate a model using k-fold cross-validation ----------------------\n",
        "def train_evaluate_model(dataX, dataY, n_folds=5):       # n-folds define the number of cycles\n",
        " trainX, trainY, testX, testY = load_dataset()           # load dataset\n",
        " trainX, testX = prep_pixels(trainX, testX)              # scale pixels\n",
        " scores, histories = list(), list()                      # initialize a empty list\n",
        " kfold = KFold(n_folds, shuffle=True, random_state=1)    # prepare cross validation\n",
        " for train_ix, test_ix in kfold.split(trainX):           # kfold.split(trainX) is a method that give the number of splits\n",
        "    model = define_model()                               # define model\n",
        "    trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]           # select rows for train and test\n",
        "    history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=2)   # Trains the model for a fixed number of epochs (dataset iterations).\n",
        "    #returns (history=)training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
        "    _, acc = model.evaluate(testX, testY, verbose=0)     # evaluate model\n",
        "    print('> %.3f' % (acc * 100.0))\n",
        "    scores.append(acc)                                   # store accuracy value in a list called scores\n",
        "    histories.append(history)                            # store history value in a list called histories\n",
        " model.save('final_model.h5')\n",
        " files.download('final_model.h5')\n",
        "# shutil.copy('/content/final_model.h5','/content/gdrive/My Drive/pruebas/') #save model to drive\n",
        " return scores, histories"
      ],
      "metadata": {
        "id": "P42ykL_psm5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- plot diagnostic learning curves -----------------------------------\n",
        "def summarize_model(histories,scores):\n",
        " for i in range(len(histories)):\n",
        "  plt.subplot(2, 1, 1)                 # plot loss\n",
        "  plt.title('Cross Entropy Loss')\n",
        "  plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
        "  plt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
        "  plt.subplot(2, 1, 2)                 # plot accuracy\n",
        "  plt.title('Classification Accuracy')\n",
        "  plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
        "  plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
        " plt.show()\n",
        " print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))   # print summary\n",
        " plt.boxplot(scores)  # box and whisker plots of results\n",
        " plt.show()"
      ],
      "metadata": {
        "id": "DdCtwHmZsqhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------ run the test for evaluating a model --------------------------\n",
        "def run_evaluation():\n",
        " trainX, trainY, testX, testY = load_dataset()             # load dataset\n",
        " trainX, testX = prep_pixels(trainX, testX)   \t           # prepare pixel data\n",
        " scores, histories = train_evaluate_model(trainX, trainY)  # evaluate model\n",
        " return scores, histories\n",
        "\n",
        "# summarize_model(histories,scores)                         # summarize performance curves"
      ],
      "metadata": {
        "id": "vYDneCOVstIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- load and prepare the new image -----------------------------------\n",
        "def load_image(filename):\n",
        " #  data_folder=Path(\"sample_data/\")    #filename=data_folder/filename\n",
        " img = load_img(filename, grayscale=True, target_size=(28, 28)) # load the image\n",
        " img = img_to_array(img)                                        # convert to array\n",
        " img = img.reshape(1, 28, 28, 1)                                # reshape into a single sample with 1 channel\n",
        " img = img.astype('float32')                                    # prepare pixel data\n",
        " img = img / 255.0\n",
        " return img"
      ],
      "metadata": {
        "id": "pbdAenpKswj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- load an image and predict the class --------------------------------\n",
        "def predict():\n",
        " model = load_model('final_model.h5')\n",
        " imagen = files.upload()                    # button for choosin image\n",
        " img = load_image(list(imagen.keys())[0])   # call load_image function\n",
        "#  img = load_image('sample1.png')\n",
        " predict_value = model.predict(img)         # predict the class\n",
        " digit = argmax(predict_value)              # get the max probab class\n",
        " print('Predicted digit is:  %i' % (digit))"
      ],
      "metadata": {
        "id": "uqR3cIpnsyuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3r7hkJNZVj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f2f8f4-647e-405e-aaae-741d366fe46c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n"
          ]
        }
      ],
      "source": [
        "#-----------------------------------------------------------------------------\n",
        "#--------------------------- MAIN CODE ---------------------------------------\n",
        "#-----------------------------------------------------------------------------\n",
        "#drive.mount('/content/gdrive')              # mount drive\n",
        "if trai==1:                                  # if we decide to train the model again\n",
        "\t   scores, histories =run_evaluation()        # fit and evaluate model accuracy\n",
        "# else:\n",
        "\n",
        "try:\n",
        "    model = load_model('final_model.h5')\n",
        "except:                                      # if we decide to load the trainned model from local file\n",
        "    print('PLEASE SELECT TRAINNED MODEL .H5')\n",
        "    modelo = files.upload()                    # button for choosin model\n",
        "    model = load_model(list(modelo.keys())[0]) # load the selected model\n",
        "\n",
        "summarize_model(histories,scores)\n",
        "#model. summary()\n",
        "\n",
        "#shutil.copy('/content/gdrive/MyDrive/pruebas/final_model.h5','/content/')     # load model from drive\n",
        "#model = load_model('final_model.h5')\n",
        "\n",
        "# --- new images in grayscale, handwritten digit centered aligned, size is 28×28 pixels (square) --------\n",
        "print('PLEASE SELECT NEW IMAGE FOR PREDICT DIGIT')\n",
        "if pred==1:\n",
        " predict()                            # run example of 1 photo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tuB-YkN3rllP"
      }
    }
  ]
}